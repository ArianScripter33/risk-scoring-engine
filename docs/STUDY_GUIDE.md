# ğŸ“š GuÃ­a de Estudio: Risk Scoring Engine
## De Data Scientist (Notebooks) a ML Engineer

> **Objetivo:** Entender profundamente cÃ³mo se construye un sistema de ML end-to-end en producciÃ³n, desde ETL hasta deployment.

---

## ğŸ—ºï¸ Roadmap de Aprendizaje

```mermaid
graph LR
    A[Fase 1: Entender] --> B[Fase 2: Mejorar]
    B --> C[Fase 3: Escalar en GCP]
    C --> D[Fase 4: Agentes AI]
    
    A --> A1[Pipeline Local]
    A --> A2[DVC]
    A --> A3[API REST]
    
    B --> B1[HPO]
    B --> B2[MÃ¡s Modelos]
    B --> B3[Testing]
    
    C --> C1[BigQuery ETL]
    C --> C2[Vertex AI]
    C --> C3[Cloud Run]
    
    D --> D1[Agentes de AutomatizaciÃ³n]
```

---

## ğŸ“– FASE 1: ENTENDER EL SISTEMA ACTUAL

### ğŸ” DÃ­a 1-2: La Arquitectura General

#### **Â¿QuÃ© diferencia este proyecto de un Notebook?**

| Aspecto | Notebook Tradicional | Este Proyecto (MLOps) |
|---------|---------------------|----------------------|
| **CÃ³digo** | Todo en un solo archivo .ipynb | Modular (src/data, src/features, src/models) |
| **Reproducibilidad** | Manual, dependiente del orden de celdas | Automatizada con DVC pipelines |
| **Datos** | Cargados en memoria cada vez | Versionados y cacheados por DVC |
| **Modelo** | Se entrena cada vez | Solo se reentrena si cambian datos/cÃ³digo |
| **Deployment** | No hay | API REST lista para producciÃ³n |
| **ColaboraciÃ³n** | DifÃ­cil (conflictos en .ipynb) | CÃ³digo Python estÃ¡ndar + Git |

#### **Ejercicio 1: Mapea el Flujo de Datos**

Abre estos archivos y responde:

1. **`dvc.yaml`** (lÃ­neas 1-28)
   - Â¿CuÃ¡ntas etapas tiene el pipeline?
   - Â¿QuÃ© archivos de entrada necesita cada etapa?
   - Â¿QuÃ© salidas produce cada etapa?

2. **Diagrama Mental:**
   ```
   data/01_raw/*.csv 
         â†“ (process_data)
   data/03_primary/credit_data_processed.csv
         â†“ (engineer_features)
   data/04_features/*.npy + feature_pipeline.pkl
         â†“ (train_model)
   models/credit_risk_model_*.pkl
   ```

**ğŸ¯ Checkpoint:** Puedes explicar quÃ© archivo genera cada etapa y por quÃ©.

---

### ğŸ”§ DÃ­a 3-4: Profundizando en el Pipeline de Datos

#### **Archivo Clave: `src/data/make_dataset.py`**

Este es tu primer ejemplo de **Data Engineering en Python puro**.

##### **Conceptos a Entender:**

1. **Modularidad (lÃ­neas 23-50):**
   ```python
   def create_dummy_data() -> tuple:
       """Una funciÃ³n, una responsabilidad"""
   ```
   - â“ Â¿Por quÃ© separar en funciones en lugar de un script lineal?
   - ğŸ’¡ **Respuesta:** Testeable, reutilizable, debuggeable

2. **Logging Profesional (lÃ­neas 16-20):**
   ```python
   logging.basicConfig(level=logging.INFO)
   logger.info("Cargando datos...")
   ```
   - â“ Â¿Por quÃ© no usar `print()`?
   - ğŸ’¡ **Respuesta:** Los logs tienen timestamps, niveles (INFO/ERROR), se pueden redirigir a archivos/cloud

3. **Agregaciones SQL-like en Pandas (lÃ­neas 68-74):**
   ```python
   bureau_agg = df_bureau.groupby('SK_ID_CURR').agg({
       'DAYS_CREDIT': ['mean', 'max', 'min'],
   })
   ```
   - â“ Â¿QuÃ© hace esto en tÃ©rminos de SQL?
   - ğŸ’¡ **Respuesta:** `SELECT SK_ID_CURR, AVG(DAYS_CREDIT), MAX(DAYS_CREDIT) ... GROUP BY SK_ID_CURR`

##### **Ejercicio 2: Debugging PrÃ¡ctico**

```bash
# Ejecuta solo esta etapa del pipeline
dvc repro process_data

# Mira los logs
cat .dvc/tmp/process_data.log

# Inspecciona el output
head -20 data/03_primary/credit_data_processed.csv
```

**ğŸ¯ Checkpoint:** Entiendes cÃ³mo se unen mÃºltiples tablas (application_train + bureau) y por quÃ© esto es parecido a ETL.

---

### âš™ï¸ DÃ­a 5-6: Feature Engineering

#### **Archivo Clave: `src/features/build_features.py`**

Este archivo es **crÃ­tico** para ML en producciÃ³n.

##### **Conceptos a Entender:**

1. **Pipelines de Sklearn (lÃ­neas 62-92):**
   ```python
   numeric_transformer = Pipeline([
       ('imputer', SimpleImputer(strategy='median')),
       ('scaler', StandardScaler())
   ])
   ```
   - â“ Â¿Por quÃ© no hacer `df['col'] = (df['col'] - mean) / std`?
   - ğŸ’¡ **Respuesta:** El pipeline se entrena en train y se aplica a test/producciÃ³n **con los mismos parÃ¡metros**

2. **fit_transform vs transform:**
   ```python
   # En entrenamiento:
   X_transformed = pipeline.fit_transform(X_train)
   
   # En producciÃ³n (API):
   X_transformed = pipeline.transform(X_new)
   ```
   - â“ Â¿Por quÃ© guardamos `feature_pipeline.pkl`?
   - ğŸ’¡ **Respuesta:** Para aplicar **exactamente las mismas transformaciones** a datos nuevos

3. **Feature Engineering (lÃ­neas 48-60):**
   ```python
   df['CREDIT_INCOME_PERCENT'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']
   ```
   - â“ Â¿Por quÃ© crear estas features derivadas?
   - ğŸ’¡ **Respuesta:** Ratios financieros son mÃ¡s informativos que valores absolutos para riesgo crediticio

##### **Ejercicio 3: Experimentos con Features**

Modifica `build_features.py` para agregar una nueva feature:

```python
# LÃ­nea 54 (despuÃ©s de CREDIT_INCOME_PERCENT)
df['AGE_YEARS'] = -df['DAYS_BIRTH'] / 365
df['EMPLOYMENT_YEARS'] = -df['DAYS_EMPLOYED'] / 365
```

Ejecuta:
```bash
dvc repro engineer_features
# DVC detecta el cambio y re-ejecuta desde aquÃ­
```

**ğŸ¯ Checkpoint:** Entiendes la diferencia entre transformar datos en un notebook (temporal) vs. crear un pipeline reutilizable.

---

### ğŸ¤– DÃ­a 7-8: Entrenamiento del Modelo

#### **Archivo Clave: `src/models/train_model.py`**

##### **Conceptos a Entender:**

1. **Clases para Organizar CÃ³digo (lÃ­neas 27-96):**
   ```python
   class CreditRiskModel:
       def load_data(self):
       def train(self):
       def validate(self):
   ```
   - â“ Â¿Por quÃ© una clase en lugar de funciones sueltas?
   - ğŸ’¡ **Respuesta:** El modelo mantiene estado (self.model, self.X_train), es mÃ¡s fÃ¡cil extender

2. **ValidaciÃ³n Cruzada (lÃ­neas 72-78):**
   ```python
   cv_scores = cross_val_score(model, X, y, cv=5, scoring='roc_auc')
   ```
   - â“ Â¿Por quÃ© no solo train_test_split?
   - ğŸ’¡ **Respuesta:** CV es mÃ¡s robusto, usa mÃºltiples splits para evaluar

3. **SerializaciÃ³n de Modelos (lÃ­neas 87-95):**
   ```python
   joblib.dump(self.model, "model.pkl")
   ```
   - â“ Â¿Por quÃ© .pkl en lugar de guardar cÃ³digo?
   - ğŸ’¡ **Respuesta:** El modelo incluye pesos entrenados, no solo arquitectura

##### **Ejercicio 4: Cambiar el Tipo de Modelo**

Edita `params.yaml`:
```yaml
models:
  model_type: random_forest  # Cambia de logistic_regression
```

Ejecuta:
```bash
dvc repro train_model
```

Compara los AUC scores. Â¿CuÃ¡l es mejor?

**ğŸ¯ Checkpoint:** Entiendes cÃ³mo parametrizar experimentos sin cambiar cÃ³digo.

---

### ğŸŒ DÃ­a 9-10: API REST para Inferencia

#### **Archivo Clave: `src/api/main.py`**

Este es el **puente entre ML y software engineering**.

##### **Conceptos a Entender:**

1. **FastAPI Basics (lÃ­neas 17-21):**
   ```python
   app = FastAPI(title="Credit Risk Scoring API")
   
   @app.post("/score")
   async def predict_score(data: ClientData):
   ```
   - â“ Â¿Por quÃ© async?
   - ğŸ’¡ **Respuesta:** Permite manejar mÃºltiples requests concurrentes

2. **Pydantic Models (lÃ­neas 24-37):**
   ```python
   class ClientData(BaseModel):
       AMT_INCOME_TOTAL: float
       AMT_CREDIT: float
   ```
   - â“ Â¿Por quÃ© definir un esquema?
   - ğŸ’¡ **Respuesta:** ValidaciÃ³n automÃ¡tica, documentaciÃ³n generada, type safety

3. **Carga de Modelos en Startup (lÃ­neas 46-62):**
   ```python
   @app.on_event("startup")
   async def startup_event():
       model = joblib.load("model.pkl")
   ```
   - â“ Â¿Por quÃ© cargar en startup y no en cada request?
   - ğŸ’¡ **Respuesta:** Cargar el modelo es lento (segundos), solo se hace una vez

##### **Ejercicio 5: Prueba la API Localmente**

```bash
# Terminal 1: Levanta la API
uvicorn src.api.main:app --reload

# Terminal 2: Haz un request
curl -X POST http://localhost:8000/score \
  -H "Content-Type: application/json" \
  -d '{
    "AMT_INCOME_TOTAL": 150000,
    "AMT_CREDIT": 300000,
    "AMT_ANNUITY": 15000,
    "DAYS_BIRTH": -12000,
    "DAYS_EMPLOYED": -2000
  }'
```

Ve a http://localhost:8000/docs para la documentaciÃ³n interactiva.

**ğŸ¯ Checkpoint:** Entiendes cÃ³mo un modelo .pkl se convierte en un servicio HTTP.

---

### ğŸ”„ DÃ­a 11-12: DVC - El NÃºcleo de MLOps

#### **Â¿Por quÃ© DVC?**

| Sin DVC | Con DVC |
|---------|---------|
| `python script1.py && python script2.py && ...` | `dvc repro` |
| Re-ejecuta todo siempre | Solo re-ejecuta lo que cambiÃ³ |
| Datos no versionados | Datos en .dvc tracked by git |
| No hay cachÃ© | CachÃ© inteligente de outputs |

##### **Conceptos a Entender:**

1. **Dependencias (deps) y Salidas (outs):**
   ```yaml
   stages:
     process_data:
       cmd: python src/data/make_dataset.py
       deps:
         - src/data/make_dataset.py  # Si cambia el cÃ³digo
         - data/01_raw               # O si cambian los datos
       outs:
         - data/03_primary           # DVC cachea esto
   ```

2. **dvc.lock (estado del pipeline):**
   ```yaml
   process_data:
     cmd: python ...
     deps:
     - md5: a1b2c3d4  # Hash del cÃ³digo
     outs:
     - md5: e5f6g7h8  # Hash de los datos generados
   ```
   - DVC compara estos hashes para saber si necesita re-ejecutar

##### **Ejercicio 6: Experimenta con el CachÃ©**

```bash
# Ejecuta el pipeline
dvc repro

# Borra los outputs
rm -rf data/03_primary data/04_features models/*.pkl

# Recupera desde el cachÃ© (sin re-ejecutar)
dvc checkout

# Verifica que los archivos estÃ¡n de vuelta
ls -lh models/
```

**ğŸ¯ Checkpoint:** Entiendes que DVC es como Git pero para datos/modelos grandes.

---

## ğŸ“Š RESUMEN DE FASE 1

### Conceptos Clave Aprendidos:

âœ… **ETL en Python:** Agregaciones, joins, transformaciones modulares  
âœ… **Feature Pipelines:** fit/transform, serializaciÃ³n, reproducibilidad  
âœ… **Entrenamiento Parametrizado:** YAML configs, validaciÃ³n cruzada  
âœ… **API REST:** FastAPI, Pydantic, model serving  
âœ… **OrquestaciÃ³n:** DVC para pipelines reproducibles  

### Diferencias con Notebook:

| Notebook | Proyecto MLOps |
|----------|----------------|
| ExploraciÃ³n | ProducciÃ³n |
| Manual | Automatizado |
| CÃ³digo lineal | Modular |
| No reproducible | Reproducible |
| Solo local | Deploy-ready |

---

## ğŸš€ PRÃ“XIMOS PASOS

Una vez completes esta fase, estarÃ¡s listo para:

### **Fase 2: Mejoras Locales (1-2 semanas)**
- Hyperparameter Optimization (Optuna)
- MÃ¡s modelos (XGBoost, LightGBM)
- MÃ©tricas avanzadas (Gini, KS-Statistic)
- Testing (pytest)

### **Fase 3: Escalar a GCP (2-3 semanas)**
- BigQuery para ETL de datos grandes
- Vertex AI Pipelines (DVC â†’ Vertex)
- Cloud Run para deployment
- Vertex AI Model Registry

### **Fase 4: Agentes de AutomatizaciÃ³n (Avanzado)**
- Agentes que escriben cÃ³digo de feature engineering
- Agentes que optimizan hiperparÃ¡metros
- Agentes que monitorean drift y reentrenan

---

## ğŸ“ Ejercicio Final de Fase 1

**Proyecto Mini:** Crea una versiÃ³n simplificada del pipeline para otro dataset (ej. Titanic de Kaggle):

1. Crea `src/data/make_titanic.py`
2. Define un pipeline de DVC
3. Entrena un modelo
4. Crea una API que predice supervivencia
5. Todo versionado con DVC

Tiempo estimado: 4-6 horas

**Â¿Listo para empezar?** Comienza con el DÃ­a 1-2 y ve completando los ejercicios. Cada checkpoint es crucial antes de avanzar.

---

## ğŸ¤” Preguntas para Reflexionar

DespuÃ©s de esta fase, deberÃ­as poder responder:

1. Â¿Por quÃ© `dvc repro` es mejor que `bash run_all.sh`?
2. Â¿Por quÃ© guardamos `feature_pipeline.pkl` ademÃ¡s del modelo?
3. Â¿QuÃ© pasa si cambio una feature en producciÃ³n sin reentrenar?
4. Â¿CÃ³mo se verÃ­a esto con millones de filas? (respuesta: BigQuery)
5. Â¿CÃ³mo automatizar el reentrenamiento? (respuesta: CI/CD + Vertex AI)

Si puedes responder estas 5, estÃ¡s listo para Fase 2 y GCP.
