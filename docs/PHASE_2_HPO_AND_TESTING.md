# ðŸš€ Fase 2: Mejoras Locales - HPO y Testing
## Hyperparameter Optimization + Modelos Avanzados + Testing

> **Pre-requisito:** Completar Fase 1 (entender el pipeline actual)

---

## ðŸŽ¯ Objetivos de Esta Fase

1. âœ… Implementar **Hyperparameter Optimization (HPO)** con Optuna
2. âœ… Agregar **XGBoost y LightGBM** al proyecto
3. âœ… MÃ©tricas avanzadas de riesgo: **Gini, KS-Statistic**
4. âœ… Escribir **tests automatizados** con pytest
5. âœ… Integrar **MLflow** para experiment tracking

**DuraciÃ³n estimada:** 1-2 semanas (2-3 horas diarias)

---

## ðŸ“š Parte 1: Hyperparameter Optimization (HPO)

### Â¿Por quÃ© HPO?

| Sin HPO | Con HPO |
|---------|---------|
| `n_estimators=100` (valor arbitrario) | `n_estimators=523` (optimizado) |
| AUC = 0.72 | AUC = 0.78 |
| Depende de la intuiciÃ³n del DS | BÃºsqueda sistemÃ¡tica |

### TeorÃ­a RÃ¡pida

**Optuna** es una librerÃ­a de HPO que:
- Prueba diferentes combinaciones de hiperparÃ¡metros
- Usa algoritmos inteligentes (Bayesian Optimization) en lugar de Grid Search
- Es mÃ¡s eficiente que probar todo manualmente

### Ejercicio 1: Agregar Optuna al Proyecto

#### Paso 1: Instalar Optuna

```bash
# Agrega a requirements.txt
echo "optuna>=3.0.0" >> requirements.txt
pip install optuna
```

#### Paso 2: Crear el MÃ³dulo de HPO

Crea `src/models/hyperparameter_tuning.py`:

```python
#!/usr/bin/env python3
"""
OptimizaciÃ³n de hiperparÃ¡metros con Optuna.
"""

import logging
import optuna
from optuna.integration import OptunaSearchCV
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
import lightgbm as lgb
import xgboost as xgb

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class HyperparameterOptimizer:
    """
    Clase para optimizaciÃ³n de hiperparÃ¡metros usando Optuna.
    """
    
    def __init__(self, X_train, y_train, model_type='random_forest', n_trials=50):
        """
        Args:
            X_train: Features de entrenamiento
            y_train: Target de entrenamiento
            model_type: Tipo de modelo ('random_forest', 'xgboost', 'lightgbm')
            n_trials: NÃºmero de iteraciones de bÃºsqueda
        """
        self.X_train = X_train
        self.y_train = y_train
        self.model_type = model_type
        self.n_trials = n_trials
        self.best_params = None
        self.best_score = None
        
    def objective_random_forest(self, trial):
        """FunciÃ³n objetivo para Random Forest."""
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 50, 1000),
            'max_depth': trial.suggest_int('max_depth', 3, 50),
            'min_samples_split': trial.suggest_int('min_samples_split', 2, 32),
            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 32),
            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),
            'class_weight': 'balanced',
            'random_state': 42,
            'n_jobs': -1
        }
        
        model = RandomForestClassifier(**params)
        
        # ValidaciÃ³n cruzada con 3 folds (mÃ¡s rÃ¡pido que 5 para HPO)
        scores = cross_val_score(
            model, self.X_train, self.y_train, 
            cv=3, scoring='roc_auc', n_jobs=-1
        )
        
        return scores.mean()
    
    def objective_xgboost(self, trial):
        """FunciÃ³n objetivo para XGBoost."""
        params = {
            'objective': 'binary:logistic',
            'eval_metric': 'auc',
            'n_estimators': trial.suggest_int('n_estimators', 100, 2000),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
            'max_depth': trial.suggest_int('max_depth', 3, 12),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
            'subsample': trial.suggest_float('subsample', 0.5, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
            'random_state': 42
        }
        
        model = xgb.XGBClassifier(**params)
        
        scores = cross_val_score(
            model, self.X_train, self.y_train,
            cv=3, scoring='roc_auc', n_jobs=-1
        )
        
        return scores.mean()
    
    def objective_lightgbm(self, trial):
        """FunciÃ³n objetivo para LightGBM."""
        params = {
            'objective': 'binary',
            'metric': 'auc',
            'n_estimators': trial.suggest_int('n_estimators', 100, 2000),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
            'num_leaves': trial.suggest_int('num_leaves', 20, 300),
            'max_depth': trial.suggest_int('max_depth', 3, 12),
            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
            'subsample': trial.suggest_float('subsample', 0.5, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
            'class_weight': 'balanced',
            'random_state': 42,
            'verbose': -1
        }
        
        model = lgb.LGBMClassifier(**params)
        
        scores = cross_val_score(
            model, self.X_train, self.y_train,
            cv=3, scoring='roc_auc', n_jobs=-1
        )
        
        return scores.mean()
    
    def optimize(self):
        """Ejecuta la optimizaciÃ³n de hiperparÃ¡metros."""
        logger.info(f"Iniciando optimizaciÃ³n para {self.model_type} con {self.n_trials} trials")
        
        # Seleccionar la funciÃ³n objetivo segÃºn el tipo de modelo
        objective_map = {
            'random_forest': self.objective_random_forest,
            'xgboost': self.objective_xgboost,
            'lightgbm': self.objective_lightgbm
        }
        
        if self.model_type not in objective_map:
            raise ValueError(f"Modelo {self.model_type} no soportado")
        
        # Crear estudio de Optuna
        study = optuna.create_study(
            direction='maximize',
            sampler=optuna.samplers.TPESampler(seed=42)
        )
        
        # Optimizar
        study.optimize(
            objective_map[self.model_type],
            n_trials=self.n_trials,
            show_progress_bar=True
        )
        
        self.best_params = study.best_params
        self.best_score = study.best_value
        
        logger.info(f"Mejores parÃ¡metros encontrados: {self.best_params}")
        logger.info(f"Mejor AUC-ROC: {self.best_score:.4f}")
        
        return self.best_params, self.best_score


if __name__ == "__main__":
    # Ejemplo de uso
    import numpy as np
    from pathlib import Path
    
    # Cargar datos
    data_path = Path("data/04_features")
    X = np.load(data_path / "X_features.npy", allow_pickle=True)
    y = np.load(data_path / "y_target.npy", allow_pickle=True)
    
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # Optimizar Random Forest
    optimizer = HyperparameterOptimizer(X_train, y_train, model_type='random_forest', n_trials=20)
    best_params, best_score = optimizer.optimize()
```

#### Paso 3: Modificar `train_model.py` para Usar HPO

Edita `src/models/train_model.py` para agregar un flag de HPO:

```python
# AÃ±adir al inicio
from src.models.hyperparameter_tuning import HyperparameterOptimizer

# En la clase CreditRiskModel, agregar mÃ©todo:
def optimize_hyperparameters(self, n_trials=50):
    """Optimiza hiperparÃ¡metros antes del entrenamiento."""
    logger.info("Optimizando hiperparÃ¡metros...")
    
    optimizer = HyperparameterOptimizer(
        self.X_train, self.y_train,
        model_type=self.model_type,
        n_trials=n_trials
    )
    
    best_params, best_score = optimizer.optimize()
    self.best_hyperparameters = best_params
    
    logger.info(f"HPO completado. Mejor AUC: {best_score:.4f}")
    return best_params

# Modificar el mÃ©todo create_model para usar los mejores parÃ¡metros:
def create_model(self) -> None:
    """Crea el modelo segÃºn el tipo especificado."""
    logger.info(f"Creando modelo: {self.model_type}")
    
    if self.model_type == 'random_forest':
        if hasattr(self, 'best_hyperparameters'):
            self.model = RandomForestClassifier(**self.best_hyperparameters)
        else:
            self.model = RandomForestClassifier(
                n_estimators=100, random_state=42, class_weight='balanced'
            )
    # ... resto del cÃ³digo
```

#### Paso 4: Actualizar `params.yaml` para HPO

```yaml
models:
  model_type: random_forest
  use_hpo: true
  n_trials: 50  # Ajusta segÃºn tu tiempo disponible
```

### ðŸŽ¯ Checkpoint 1

Ejecuta HPO y compara resultados:

```bash
# Sin HPO (baseline)
python src/models/train_model.py

# Con HPO
# Edita params.yaml: use_hpo: true
python src/models/train_model.py
```

**Pregunta:** Â¿CuÃ¡nto mejorÃ³ el AUC? Â¿ValiÃ³ la pena el tiempo extra?

---

## ðŸ“š Parte 2: Modelos Avanzados (XGBoost, LightGBM)

### Â¿Por quÃ© XGBoost/LightGBM?

| Logistic Regression | Random Forest | XGBoost/LightGBM |
|---------------------|---------------|------------------|
| RÃ¡pido | Moderado | Moderado-Lento |
| No captura interacciones complejas | Captura interacciones | Captura interacciones + regularizaciÃ³n |
| AUC tÃ­pico: 0.70-0.75 | AUC tÃ­pico: 0.75-0.80 | AUC tÃ­pico: 0.78-0.85 |

### Ejercicio 2: Agregar XGBoost y LightGBM

#### Paso 1: Instalar librerÃ­as

```bash
echo "xgboost>=2.0.0" >> requirements.txt
echo "lightgbm>=4.0.0" >> requirements.txt
pip install xgboost lightgbm
```

#### Paso 2: Modificar `train_model.py`

```python
# AÃ±adir imports
import xgboost as xgb
import lightgbm as lgb

# En la clase CreditRiskModel, modificar create_model:
def create_model(self) -> None:
    """Crea el modelo segÃºn el tipo especificado."""
    logger.info(f"Creando modelo: {self.model_type}")
    
    params = self.best_hyperparameters if hasattr(self, 'best_hyperparameters') else {}
    
    if self.model_type == 'logistic_regression':
        self.model = LogisticRegression(
            random_state=42, max_iter=1000, class_weight='balanced'
        )
    elif self.model_type == 'random_forest':
        default_params = {'n_estimators': 100, 'random_state': 42, 'class_weight': 'balanced'}
        self.model = RandomForestClassifier(**{**default_params, **params})
    
    elif self.model_type == 'xgboost':
        default_params = {
            'n_estimators': 500,
            'learning_rate': 0.1,
            'max_depth': 6,
            'random_state': 42,
            'eval_metric': 'auc'
        }
        self.model = xgb.XGBClassifier(**{**default_params, **params})
    
    elif self.model_type == 'lightgbm':
        default_params = {
            'n_estimators': 500,
            'learning_rate': 0.1,
            'num_leaves': 31,
            'random_state': 42,
            'class_weight': 'balanced',
            'verbose': -1
        }
        self.model = lgb.LGBMClassifier(**{**default_params, **params})
    
    else:
        raise ValueError(f"Tipo de modelo no soportado: {self.model_type}")
```

#### Paso 3: Experimento Comparativo

Crea un script para comparar todos los modelos:

```bash
# Crea src/models/compare_models.py
cat > src/models/compare_models.py << 'EOF'
#!/usr/bin/env python3
"""
Compara mÃºltiples modelos y guarda resultados.
"""

import yaml
import pandas as pd
from train_model import CreditRiskModel

# Modelos a probar
models = ['logistic_regression', 'random_forest', 'xgboost', 'lightgbm']

results = []

for model_type in models:
    print(f"\n{'='*60}")
    print(f"Entrenando: {model_type}")
    print(f"{'='*60}\n")
    
    # Crear y entrenar modelo
    credit_model = CreditRiskModel(model_type=model_type)
    credit_model.load_data()
    credit_model.train()
    metrics = credit_model.validate()
    
    results.append({
        'model': model_type,
        'cv_auc': metrics['cv_auc_mean'],
        'test_auc': metrics['test_auc']
    })

# Guardar resultados
df_results = pd.DataFrame(results)
df_results.to_csv('models/model_comparison.csv', index=False)

print("\n" + "="*60)
print("RESULTADOS FINALES")
print("="*60)
print(df_results.to_string(index=False))
print(f"\nMejor modelo: {df_results.loc[df_results['test_auc'].idxmax(), 'model']}")
EOF

python src/models/compare_models.py
```

### ðŸŽ¯ Checkpoint 2

**Preguntas:**
1. Â¿QuÃ© modelo dio el mejor AUC?
2. Â¿CuÃ¡l fue el trade-off entre tiempo de entrenamiento y performance?
3. Â¿Vale la pena usar XGBoost si solo mejora 0.02 de AUC pero tarda 10x mÃ¡s?

---

## ðŸ“š Parte 3: MÃ©tricas Avanzadas de Riesgo

### TeorÃ­a: MÃ©tricas EspecÃ­ficas para Credit Scoring

| MÃ©trica | QuÃ© Mide | Por QuÃ© Importa |
|---------|----------|-----------------|
| **Gini** | Desigualdad en la distribuciÃ³n de scores | Gini alto = modelo separa bien buenos/malos |
| **KS-Statistic** | MÃ¡xima separaciÃ³n entre distribuciones | KS alto = hay un threshold Ã³ptimo claro |
| **Precision-Recall AUC** | Performance en clase minoritaria | MÃ¡s relevante que ROC-AUC en datos desbalanceados |

### Ejercicio 3: Implementar MÃ©tricas Avanzadas

Crea `src/models/risk_metrics.py`:

```python
#!/usr/bin/env python3
"""
MÃ©tricas especÃ­ficas para scoring de riesgo crediticio.
"""

import numpy as np
from sklearn.metrics import roc_auc_score, precision_recall_curve, auc
from scipy import stats


def gini_coefficient(y_true, y_pred_proba):
    """
    Calcula el coeficiente de Gini.
    
    Gini = 2 * AUC - 1
    
    InterpretaciÃ³n:
    - 0: No hay separaciÃ³n (modelo aleatorio)
    - 1: SeparaciÃ³n perfecta
    - TÃ­pico en credit scoring: 0.3-0.6
    """
    auc_score = roc_auc_score(y_true, y_pred_proba)
    gini = 2 * auc_score - 1
    return gini


def ks_statistic(y_true, y_pred_proba):
    """
    Calcula el estadÃ­stico de Kolmogorov-Smirnov.
    
    Mide la mÃ¡xima separaciÃ³n entre las distribuciones acumuladas
    de buenos y malos pagadores.
    
    InterpretaciÃ³n:
    - 0-0.2: Pobre
    - 0.2-0.3: Aceptable
    - 0.3-0.5: Bueno
    - >0.5: Excelente
    """
    # Separar scores por clase
    scores_class_0 = y_pred_proba[y_true == 0]
    scores_class_1 = y_pred_proba[y_true == 1]
    
    # Calcular KS statistic
    ks_stat, p_value = stats.ks_2samp(scores_class_0, scores_class_1)
    
    return ks_stat


def precision_recall_auc(y_true, y_pred_proba):
    """
    Calcula el Ã¡rea bajo la curva Precision-Recall.
    
    MÃ¡s relevante que ROC-AUC para datasets desbalanceados.
    """
    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)
    pr_auc = auc(recall, precision)
    return pr_auc


def calculate_all_risk_metrics(y_true, y_pred_proba):
    """
    Calcula todas las mÃ©tricas de riesgo.
    
    Returns:
        dict: Diccionario con todas las mÃ©tricas
    """
    metrics = {
        'roc_auc': roc_auc_score(y_true, y_pred_proba),
        'gini': gini_coefficient(y_true, y_pred_proba),
        'ks_statistic': ks_statistic(y_true, y_pred_proba),
        'pr_auc': precision_recall_auc(y_true, y_pred_proba)
    }
    
    return metrics


def print_risk_metrics(metrics, model_name="Model"):
    """Imprime mÃ©tricas de forma formateada."""
    print(f"\n{'='*60}")
    print(f"MÃ‰TRICAS DE RIESGO: {model_name}")
    print(f"{'='*60}")
    print(f"ROC-AUC:       {metrics['roc_auc']:.4f}")
    print(f"Gini:          {metrics['gini']:.4f}")
    print(f"KS-Statistic:  {metrics['ks_statistic']:.4f}")
    print(f"PR-AUC:        {metrics['pr_auc']:.4f}")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    # Test con datos dummy
    np.random.seed(42)
    y_true = np.random.randint(0, 2, 1000)
    y_pred_proba = np.random.random(1000)
    
    metrics = calculate_all_risk_metrics(y_true, y_pred_proba)
    print_risk_metrics(metrics)
```

#### Integrar en `train_model.py`:

```python
# AÃ±adir import
from src.models.risk_metrics import calculate_all_risk_metrics, print_risk_metrics

# Modificar el mÃ©todo validate():
def validate(self) -> dict:
    """Realiza validaciÃ³n y evaluaciÃ³n del modelo."""
    logger.info("Realizando validaciÃ³n del modelo")
    
    # ValidaciÃ³n cruzada
    cv_scores = cross_val_score(self.model, self.X_train, self.y_train, cv=5, scoring='roc_auc')
    
    # EvaluaciÃ³n en datos de prueba
    test_proba = self.model.predict_proba(self.X_test)[:, 1]
    
    # Calcular todas las mÃ©tricas
    metrics = calculate_all_risk_metrics(self.y_test, test_proba)
    metrics['cv_auc_mean'] = cv_scores.mean()
    
    print_risk_metrics(metrics, model_name=self.model_type)
    
    return metrics
```

### ðŸŽ¯ Checkpoint 3

Ejecuta tu modelo y analiza:

```bash
python src/models/train_model.py
```

**Preguntas:**
1. Â¿Tu modelo tiene Gini > 0.3? (Bueno para credit scoring)
2. Â¿El KS-Statistic es > 0.2? (MÃ­nimo aceptable)
3. Â¿Hay mucha diferencia entre ROC-AUC y PR-AUC? (Si sÃ­, tus datos estÃ¡n muy desbalanceados)

---

## ðŸ“š Parte 4: Testing Automatizado

### Â¿Por quÃ© Testing en ML?

```
Sin Tests                    Con Tests
   â†“                            â†“
Cambias cÃ³digo          Cambias cÃ³digo
   â†“                            â†“
Â¿Funciona? ðŸ¤·           pytest â†’ âœ… o âŒ
   â†“                            â†“
Deploy a producciÃ³n     Deploy solo si âœ…
   â†“                            â†“
Bug en producciÃ³n ðŸ’¥    PrevenciÃ³n temprana
```

### Ejercicio 4: Escribir Tests con pytest

#### Paso 1: Instalar pytest

```bash
echo "pytest>=7.0.0" >> requirements.txt
pip install pytest
```

#### Paso 2: Crear Tests para Data Pipeline

Crea `tests/test_data_pipeline.py`:

```python
#!/usr/bin/env python3
"""
Tests para el pipeline de datos.
"""

import pytest
import pandas as pd
import numpy as np
from src.data.make_dataset import load_and_merge_data, create_dummy_data


def test_create_dummy_data():
    """Test que create_dummy_data genera datos vÃ¡lidos."""
    df_app, df_bureau = create_dummy_data()
    
    # Verificar shapes
    assert df_app.shape[0] == 100, "Application debe tener 100 filas"
    assert df_bureau.shape[0] == 200, "Bureau debe tener 200 filas"
    
    # Verificar columnas crÃ­ticas
    assert 'SK_ID_CURR' in df_app.columns
    assert 'TARGET' in df_app.columns
    assert 'SK_ID_CURR' in df_bureau.columns


def test_load_and_merge_data(tmp_path):
    """Test que la funciÃ³n de merge funciona correctamente."""
    # Crear datos dummy en un directorio temporal
    df_merged = load_and_merge_data("data/01_raw")  # Usa dummy data
    
    # Verificar que el merge funcionÃ³
    assert 'SK_ID_CURR' in df_merged.columns
    assert 'TARGET' in df_merged.columns
    
    # Verificar que hay columnas agregadas de bureau
    bureau_cols = [col for col in df_merged.columns if 'DAYS_CREDIT' in col or 'AMT_CREDIT_SUM' in col]
    assert len(bureau_cols) > 0, "Deben existir columnas agregadas de bureau"


def test_no_missing_target():
    """Test que TARGET no tiene valores faltantes despuÃ©s del procesamiento."""
    df_merged = load_and_merge_data("data/01_raw")
    assert df_merged['TARGET'].isna().sum() == 0, "TARGET no debe tener NaN"
```

#### Paso 3: Tests para Feature Engineering

Crea `tests/test_features.py`:

```python
#!/usr/bin/env python3
"""
Tests para feature engineering.
"""

import pytest
import pandas as pd
import numpy as np
from src.features.build_features import FeatureEngineer


@pytest.fixture
def sample_data():
    """Fixture que genera datos de ejemplo para tests."""
    data = {
        'SK_ID_CURR': range(100),
        'TARGET': np.random.randint(0, 2, 100),
        'AMT_INCOME_TOTAL': np.random.uniform(25000, 200000, 100),
        'AMT_CREDIT': np.random.uniform(50000, 500000, 100),
        'AMT_ANNUITY': np.random.uniform(5000, 50000, 100),
        'DAYS_BIRTH': np.random.randint(-25000, -7000, 100),
        'DAYS_EMPLOYED': np.random.randint(-10000, 0, 100),
    }
    return pd.DataFrame(data)


def test_feature_engineer_creates_new_features(sample_data):
    """Test que se crean las features esperadas."""
    fe = FeatureEngineer()
    df_engineered = fe.engineer_features(sample_data)
    
    # Verificar que se crearon las nuevas features
    assert 'CREDIT_INCOME_PERCENT' in df_engineered.columns
    assert 'ANNUITY_INCOME_PERCENT' in df_engineered.columns
    assert 'CREDIT_TERM' in df_engineered.columns


def test_feature_engineer_no_nan_in_ratios(sample_data):
    """Test que los ratios no generan infinitos."""
    fe = FeatureEngineer()
    df_engineered = fe.engineer_features(sample_data)
    
    # Verificar que no hay infinitos
    assert not np.isinf(df_engineered['CREDIT_INCOME_PERCENT']).any()


def test_pipeline_transform_shape(sample_data):
    """Test que el pipeline mantiene el nÃºmero de filas."""
    fe = FeatureEngineer()
    X_transformed, y = fe.fit_transform(sample_data)
    
    assert X_transformed.shape[0] == sample_data.shape[0], "El nÃºmero de filas debe mantenerse"
```

#### Paso 4: Tests para el Modelo

Crea `tests/test_model.py`:

```python
#!/usr/bin/env python3
"""
Tests para el modelo.
"""

import pytest
import numpy as np
from sklearn.datasets import make_classification
from src.models.train_model import CreditRiskModel


@pytest.fixture
def mock_data():
    """Genera datos sintÃ©ticos para testing."""
    X, y = make_classification(
        n_samples=1000, n_features=20, n_informative=10,
        n_redundant=5, random_state=42
    )
    return X, y


def test_model_training(mock_data):
    """Test que el modelo entrena sin errores."""
    X, y = mock_data
    
    # Guardar datos temporales
    import tempfile
    with tempfile.TemporaryDirectory() as tmp_dir:
        np.save(f"{tmp_dir}/X_features.npy", X)
        np.save(f"{tmp_dir}/y_target.npy", y)
        
        model = CreditRiskModel(model_type='logistic_regression')
        model.load_data(tmp_dir)
        model.train()
        
        assert model.model is not None, "El modelo debe estar entrenado"


def test_model_predictions_in_valid_range(mock_data):
    """Test que las probabilidades predichas estÃ¡n entre 0 y 1."""
    X, y = mock_data
    
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    model = CreditRiskModel(model_type='logistic_regression')
    model.X_train, model.X_test = X_train, X_test
    model.y_train, model.y_test = y_train, y_test
    model.train()
    
    proba = model.model.predict_proba(X_test)[:, 1]
    
    assert np.all(proba >= 0) and np.all(proba <= 1), "Probabilidades deben estar entre 0 y 1"


def test_model_auc_above_threshold(mock_data):
    """Test que el AUC es mayor que un threshold mÃ­nimo."""
    X, y = mock_data
    
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    model = CreditRiskModel(model_type='random_forest')
    model.X_train, model.X_test = X_train, X_test
    model.y_train, model.y_test = y_train, y_test
    model.train()
    metrics = model.validate()
    
    assert metrics['test_auc'] > 0.7, "AUC debe ser mayor a 0.7 con datos sintÃ©ticos"
```

#### Paso 5: Ejecutar Tests

```bash
# Ejecutar todos los tests
pytest tests/ -v

# Ver cobertura de cÃ³digo
pip install pytest-cov
pytest tests/ --cov=src --cov-report=html

# Abrir reporte
open htmlcov/index.html
```

### ðŸŽ¯ Checkpoint 4

**Preguntas:**
1. Â¿Todos los tests pasan? Si no, Â¿por quÃ©?
2. Â¿QuÃ© porcentaje de cobertura tienes? (Meta: >70%)
3. Â¿QuÃ© test agregarÃ­as para la API?

---

## ðŸ“Š RESUMEN DE FASE 2

### âœ… Lo que Lograste

1. **HPO con Optuna:** OptimizaciÃ³n sistemÃ¡tica de hiperparÃ¡metros
2. **Modelos Avanzados:** XGBoost y LightGBM en el pipeline
3. **MÃ©tricas de Riesgo:** Gini, KS-Statistic, PR-AUC
4. **Testing:** Suite de tests automatizados con pytest

### ðŸŽ“ Habilidades Nuevas

- Hyperparameter tuning a escala profesional
- ComparaciÃ³n objetiva de mÃºltiples modelos
- MÃ©tricas especÃ­ficas del dominio (credit scoring)
- Testing automatizado para ML

### ðŸ“ˆ Mejora TÃ­pica Esperada

| MÃ©trica | Antes (Fase 1) | DespuÃ©s (Fase 2) |
|---------|----------------|------------------|
| AUC | 0.72 | 0.78-0.82 |
| Gini | 0.44 | 0.56-0.64 |
| Cobertura Tests | 0% | 70%+ |
| Confianza en Deploy | Baja | Alta |

---

## ðŸš€ SIGUIENTE PASO: Fase 3 (GCP)

Ahora que tienes un sistema robusto localmente, es hora de escalarlo a la nube:

- BigQuery para ETL de millones de filas
- Vertex AI para entrenamiento distribuido
- Cloud Run para deployment escalable
- CI/CD automatizado con Cloud Build

**ContinÃºa en:** `PHASE_3_GCP_SCALING.md`
